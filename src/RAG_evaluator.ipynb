{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21973e47",
   "metadata": {},
   "source": [
    "# RAG evaluator\n",
    "- [x] Metrics -> on Retriever component\n",
    "- [x] Generator component -> with LLM-based judge\n",
    "- [x] evaluation dataset\n",
    "- [x] llm main style evaluation\n",
    "- [x] RAGAS https://medium.com/data-science/evaluating-rag-applications-with-ragas-81d67b0ee31a\n",
    "- [ ] better source data on detailed project and information\n",
    "- [ ] better golden dataset\n",
    "- [ ] Dashboard - Once Stable after all the other RAG part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "50693432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from base_models import TestQuestion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01c8f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "TEST_QUESTIONS_FILE = \"../evaluation/eval_data.jsonl\"\n",
    "\n",
    "def load_test_questions() -> list[TestQuestion]:\n",
    "    \"\"\"\n",
    "    Load test questions from a JSONL file\n",
    "    \"\"\"\n",
    "    with open(TEST_QUESTIONS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        tests = []\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip()) \n",
    "            tests.append(TestQuestion(**data))\n",
    "        print(\"Loaded {} test questions\".format(len(tests)))\n",
    "    return tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "73109225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 test questions\n"
     ]
    }
   ],
   "source": [
    "tests = load_test_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "25b46e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many teams benefited from Beiji’s n8n rollout and what was the financial impact?\n",
      "More than 10 teams used the automation workflows, saving about 10,000 SGD per month.\n",
      "impact\n",
      "['n8n', '10+ teams', '10k SGD', 'automation']\n"
     ]
    }
   ],
   "source": [
    "tests[0]\n",
    "print(tests[0].question)\n",
    "print(tests[0].ground_truth)\n",
    "print(tests[0].category)\n",
    "print(tests[0].keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a475d53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ai_engineering': 10,\n",
       "         'achievement': 5,\n",
       "         'engineering': 5,\n",
       "         'personality': 4,\n",
       "         'platform_engineering': 3,\n",
       "         'rag_skill': 3,\n",
       "         'lifestyle': 3,\n",
       "         'skills': 3,\n",
       "         'multi_hop': 3,\n",
       "         'impact': 2,\n",
       "         'timeline': 2,\n",
       "         'project_experience': 2,\n",
       "         'frontend': 1,\n",
       "         'education': 1,\n",
       "         'personal_profile': 1,\n",
       "         'full_stack_ai': 1,\n",
       "         'future': 1})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "count = Counter([t.category for t in tests])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d91f93e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error getting collection: Collection [67f909e5-1762-40fe-b66b-d9309eaaf2f9] does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[62]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrag_retrieval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fetch_context\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m retrieval_results = \u001b[43mfetch_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtests\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRetrieval results:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(retrieval_results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects-AI/MyBestFriend/src/rag_retrieval.py:27\u001b[39m, in \u001b[36mfetch_context\u001b[39m\u001b[34m(query)\u001b[39m\n\u001b[32m     15\u001b[39m retriever_similarity = vectorstore.as_retriever(search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: TOP_K})\n\u001b[32m     16\u001b[39m retriever_mmr = vectorstore.as_retriever(\n\u001b[32m     17\u001b[39m     search_type=\u001b[33m\"\u001b[39m\u001b[33mmmr\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m     search_kwargs={\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m     }\n\u001b[32m     22\u001b[39m )\n\u001b[32m     24\u001b[39m SYSTEM_PROMPT = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[33mYou are a helpful assistant that can answer questions about an user\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms profile and daily life.\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[33mThis response is used for a user to know more about the user\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms profile and daily life from a HR perspective.\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \n\u001b[32m     28\u001b[39m \u001b[33mYou are given a question and a context.\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[33mYou need to answer the question based on the context.\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[33mContext:\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;132;01m{context}\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_context\u001b[39m(query: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m     35\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    Fetch the context for the query from the vector store with Top K results\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects-AI/MyBestFriend/.venv/lib/python3.12/site-packages/langchain_core/retrievers.py:216\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    214\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    220\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects-AI/MyBestFriend/.venv/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:1040\u001b[39m, in \u001b[36mVectorStoreRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1038\u001b[39m kwargs_ = \u001b[38;5;28mself\u001b[39m.search_kwargs | kwargs\n\u001b[32m   1039\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1040\u001b[39m     docs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvectorstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.search_type == \u001b[33m\"\u001b[39m\u001b[33msimilarity_score_threshold\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1042\u001b[39m     docs_and_similarities = (\n\u001b[32m   1043\u001b[39m         \u001b[38;5;28mself\u001b[39m.vectorstore.similarity_search_with_relevance_scores(\n\u001b[32m   1044\u001b[39m             query, **kwargs_\n\u001b[32m   1045\u001b[39m         )\n\u001b[32m   1046\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects-AI/MyBestFriend/.venv/lib/python3.12/site-packages/langchain_chroma/vectorstores.py:748\u001b[39m, in \u001b[36mChroma.similarity_search\u001b[39m\u001b[34m(self, query, k, filter, **kwargs)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msimilarity_search\u001b[39m(\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    732\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    735\u001b[39m     **kwargs: Any,\n\u001b[32m    736\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m    737\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run similarity search with Chroma.\u001b[39;00m\n\u001b[32m    738\u001b[39m \n\u001b[32m    739\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    746\u001b[39m \u001b[33;03m        List of documents most similar to the query text.\u001b[39;00m\n\u001b[32m    747\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m748\u001b[39m     docs_and_scores = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects-AI/MyBestFriend/.venv/lib/python3.12/site-packages/langchain_chroma/vectorstores.py:849\u001b[39m, in \u001b[36mChroma.similarity_search_with_score\u001b[39m\u001b[34m(self, query, k, filter, where_document, **kwargs)\u001b[39m\n\u001b[32m    847\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    848\u001b[39m     query_embedding = \u001b[38;5;28mself\u001b[39m._embedding_function.embed_query(query)\n\u001b[32m--> \u001b[39m\u001b[32m849\u001b[39m     results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__query_collection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _results_to_docs_and_scores(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects-AI/MyBestFriend/.venv/lib/python3.12/site-packages/langchain_core/utils/utils.py:52\u001b[39m, in \u001b[36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     46\u001b[39m     msg = (\n\u001b[32m     47\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExactly one argument in each of the following\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     48\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m groups must be defined:\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     49\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     50\u001b[39m     )\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects-AI/MyBestFriend/.venv/lib/python3.12/site-packages/langchain_chroma/vectorstores.py:477\u001b[39m, in \u001b[36mChroma.__query_collection\u001b[39m\u001b[34m(self, query_texts, query_embeddings, n_results, where, where_document, **kwargs)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;129m@xor_args\u001b[39m((\u001b[33m\"\u001b[39m\u001b[33mquery_texts\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquery_embeddings\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__query_collection\u001b[39m(\n\u001b[32m    451\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m     **kwargs: Any,\n\u001b[32m    458\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Document] | chromadb.QueryResult:\n\u001b[32m    459\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Query the chroma collection.\u001b[39;00m\n\u001b[32m    460\u001b[39m \n\u001b[32m    461\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    475\u001b[39m \u001b[33;03m    See more: https://docs.trychroma.com/reference/py-collection#query\u001b[39;00m\n\u001b[32m    476\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_collection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects-AI/MyBestFriend/.venv/lib/python3.12/site-packages/chromadb/api/models/Collection.py:247\u001b[39m, in \u001b[36mCollection.query\u001b[39m\u001b[34m(self, query_embeddings, query_texts, query_images, query_uris, ids, n_results, where, where_document, include)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the n_results nearest neighbor embeddings for provided query_embeddings or query_texts.\u001b[39;00m\n\u001b[32m    212\u001b[39m \n\u001b[32m    213\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    232\u001b[39m \n\u001b[32m    233\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    235\u001b[39m query_request = \u001b[38;5;28mself\u001b[39m._validate_and_prepare_query_request(\n\u001b[32m    236\u001b[39m     query_embeddings=query_embeddings,\n\u001b[32m    237\u001b[39m     query_texts=query_texts,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m     include=include,\n\u001b[32m    245\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m query_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhere\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwhere_document\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_request\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transform_query_response(\n\u001b[32m    260\u001b[39m     response=query_results, include=query_request[\u001b[33m\"\u001b[39m\u001b[33minclude\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    261\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects-AI/MyBestFriend/.venv/lib/python3.12/site-packages/chromadb/api/rust.py:550\u001b[39m, in \u001b[36mRustBindingsAPI._query\u001b[39m\u001b[34m(self, collection_id, query_embeddings, ids, n_results, where, where_document, include, tenant, database)\u001b[39m\n\u001b[32m    534\u001b[39m filtered_ids_amount = \u001b[38;5;28mlen\u001b[39m(ids) \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m    535\u001b[39m \u001b[38;5;28mself\u001b[39m.product_telemetry_client.capture(\n\u001b[32m    536\u001b[39m     CollectionQueryEvent(\n\u001b[32m    537\u001b[39m         collection_uuid=\u001b[38;5;28mstr\u001b[39m(collection_id),\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m     )\n\u001b[32m    548\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m rust_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mwhere_document\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtenant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m QueryResult(\n\u001b[32m    563\u001b[39m     ids=rust_response.ids,\n\u001b[32m    564\u001b[39m     embeddings=rust_response.embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m    570\u001b[39m     distances=rust_response.distances,\n\u001b[32m    571\u001b[39m )\n",
      "\u001b[31mNotFoundError\u001b[39m: Error getting collection: Collection [67f909e5-1762-40fe-b66b-d9309eaaf2f9] does not exist."
     ]
    }
   ],
   "source": [
    "from rag_retrieval import fetch_context\n",
    "\n",
    "retrieval_results = fetch_context(tests[0].question)\n",
    "\n",
    "print(\"Retrieval results:\")\n",
    "print(retrieval_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc891afa",
   "metadata": {},
   "source": [
    "### Generator Part Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10e818ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from base_models import RetrievalLLMEval\n",
    "from rag_retrieval import generate_answer\n",
    "\n",
    "LLM_EVAL_PROMPT = \"\"\"\n",
    "You are a helpful assistant that can answer questions about the user's CV and hobbies.\n",
    "You are given a question and a context.\n",
    "You need to evaluate the retrieval results based on the context.\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Generated answer:\n",
    "{generated_answer}\n",
    "\n",
    "Golden answer:\n",
    "{ground_truth}\n",
    "\n",
    "Evaluation criteria:\n",
    "- Accuracy: How many of the retrieval results are correct?\n",
    "- Relevance: How relevant are the retrieval results to the question?\n",
    "- Completeness: How complete are the retrieval results?\n",
    "- Confidence: How confident are you in the retrieval results?\n",
    "- Score: The average of accuracy, relevance, completeness\n",
    "\n",
    "Return in the following format:\n",
    "{{\n",
    "    \"accuracy\": 3,\n",
    "    \"relevance\": 2,\n",
    "    \"completeness\": 4,\n",
    "    \"confidence\": 0.9,\n",
    "    \"feedback\": \"The retrieval results is not relevant to the question but correct\",\n",
    "    \"score\": 3\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0);\n",
    "\n",
    "def evaluate_response(test_question: TestQuestion) -> RetrievalLLMEval:\n",
    "    \"\"\"\n",
    "    Evaluate the LLM-Response based on the retrieval results, not on the retrieval results based on the question\n",
    "    \"\"\"\n",
    "    # get the context\n",
    "    generated_answer, retrieval_results = generate_answer(test_question.question)\n",
    "\n",
    "    # parse the messages\n",
    "    system_messages = [SystemMessage(\n",
    "        content=(\"You are an expert evaluator assessing the quality of answers. Evaluate the generated answer by comparing it to the reference answer. Only give 5/5 scores for perfect answers.\"\n",
    "                 ))]\n",
    "    user_messages = [HumanMessage(content=LLM_EVAL_PROMPT.format(question=test_question.question, generated_answer=generated_answer, ground_truth=test_question.ground_truth))]\n",
    "    messages = system_messages + user_messages\n",
    "\n",
    "    structured_llm = llm.with_structured_output(RetrievalLLMEval)\n",
    "    response_LLM_eval = structured_llm.invoke(messages)\n",
    "\n",
    "    return response_LLM_eval\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a60d8b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'base_models.RetrievalLLMEval'>\n",
      "Is RetrievalLLMEval? True\n",
      "\n",
      "Result object:\n",
      "accuracy=4.0 relevance=3.0 completeness=4.0 confidence=0.9 feedback='The retrieval results are mostly accurate and complete, but the phrasing could be more aligned with the golden answer.' score=3.75\n",
      "\n",
      "Access attributes:\n",
      "accuracy: 4.0\n",
      "relevance: 3.0\n",
      "completeness: 4.0\n",
      "score: 3.75\n",
      "confidence: 0.9\n",
      "feedback: The retrieval results are mostly accurate and complete, but the phrasing could be more aligned with the golden answer.\n"
     ]
    }
   ],
   "source": [
    "result = evaluate_response(tests[0])\n",
    "\n",
    "# Check if it's a BaseModel/RetrievalLLMEval instance\n",
    "print(\"Type:\", type(result))\n",
    "print(\"Is RetrievalLLMEval?\", isinstance(result, RetrievalLLMEval))\n",
    "print(\"\\nResult object:\")\n",
    "print(result)\n",
    "print(\"\\nAccess attributes:\")\n",
    "print(f\"accuracy: {result.accuracy}\")\n",
    "print(f\"relevance: {result.relevance}\")\n",
    "print(f\"completeness: {result.completeness}\")\n",
    "print(f\"score: {result.score}\")\n",
    "print(f\"confidence: {result.confidence}\")\n",
    "print(f\"feedback: {result.feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f41356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_LLM(tests: list[TestQuestion]) -> RetrievalLLMEval:\n",
    "    \"\"\"\n",
    "    Evaluate all the tests\n",
    "    \"\"\"\n",
    "    results = []  \n",
    "    for test in tests:\n",
    "        results.append(evaluate_response(test))\n",
    "    evaluation_result = RetrievalLLMEval(\n",
    "        accuracy=sum([result.accuracy for result in results]) / len(results),\n",
    "        relevance=sum([result.relevance for result in results]) / len(results),\n",
    "        completeness=sum([result.completeness for result in results]) / len(results),\n",
    "        score=sum([result.score for result in results]) / len(results),\n",
    "        confidence=sum([result.confidence for result in results]) / len(results),\n",
    "        feedback=\"This is the average of all the tests\",\n",
    "    )\n",
    "    return evaluation_result\n",
    "\n",
    "eval_result_LLM = evaluate_LLM(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average of all the tests from LLM evaluation on LLM answer:\n",
      "accuracy: 3.5\n",
      "relevance: 2.96\n",
      "completeness: 3.74\n",
      "score: 3.44\n",
      "confidence: 0.8320000000000001\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAverage of all the tests from LLM evaluation on LLM answer:\")\n",
    "print(f\"accuracy: {eval_result_LLM.accuracy}\")\n",
    "print(f\"relevance: {eval_result_LLM.relevance}\")\n",
    "print(f\"completeness: {eval_result_LLM.completeness}\")\n",
    "print(f\"score: {eval_result_LLM.score}\")\n",
    "print(f\"confidence: {eval_result_LLM.confidence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a0101",
   "metadata": {},
   "source": [
    "### The metric based evals on RAG retrieval result\n",
    "- MRR\n",
    "- Keyword Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1ded30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mrr(keyword:str, retrieval_results:list) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the MRR of the retrieval results,\n",
    "    mrr = 1 -> first result contains the keyword\n",
    "    mrr = 0.5 -> second result contains the keyword\n",
    "    mrr = 0 -> no result contains the keyword\n",
    "    \"\"\"\n",
    "    keyword = keyword.lower();\n",
    "    for rank, result in enumerate(retrieval_results, start=1):\n",
    "        if keyword in result.page_content.lower():\n",
    "            return 1/rank\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dd608c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_models import RetrievalEval\n",
    "\n",
    "\n",
    "def evaluate_retrieval(test: TestQuestion) -> RetrievalEval:\n",
    "    \"\"\"\n",
    "    Evaluate the retrieval results\n",
    "    \"\"\"\n",
    "\n",
    "    retrieved_docs = fetch_context(test.question)\n",
    "    mrr_scores = [evaluate_mrr(keyword, retrieved_docs) for keyword in test.keywords]# each keyword need to be calculated separately, so a list of scores\n",
    "    avg_mrr = sum(mrr_scores) / len(mrr_scores) if mrr_scores else 0.0\n",
    "\n",
    "    # Calculate keyword coverage\n",
    "    keywords_found = sum(1 for score in mrr_scores if score > 0)\n",
    "    total_keywords = len(test.keywords)\n",
    "    keyword_coverage = (keywords_found / total_keywords * 100) if total_keywords > 0 else 0.0\n",
    "\n",
    "    return RetrievalEval(\n",
    "        MRR=avg_mrr,\n",
    "        keyword_coverage=keyword_coverage,\n",
    "    )\n",
    "\n",
    "def evaluate_all(tests: list[TestQuestion]) -> RetrievalEval:\n",
    "    \"\"\"\n",
    "    Evaluate all the tests\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for test in tests:\n",
    "        results.append(evaluate_retrieval(test))\n",
    "    \n",
    "    mrr_final = sum(result.MRR for result in results) / len(results)\n",
    "    keyword_coverage_final = sum(result.keyword_coverage for result in results) / len(results)\n",
    "\n",
    "    return RetrievalEval(\n",
    "        MRR=format(mrr_final, \".2f\"),\n",
    "        keyword_coverage=format(keyword_coverage_final, \".2f\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d69b6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average of all the tests from retrieval evaluation:\n",
      "MRR: 0.5\n",
      "Keyword Coverage: 71.27% \n"
     ]
    }
   ],
   "source": [
    "eval_result_retrieval = evaluate_all(tests)\n",
    "print(\"\\nAverage of all the tests from retrieval evaluation:\")\n",
    "print(f\"MRR: {eval_result_retrieval.MRR}\")\n",
    "print(f\"Keyword Coverage: {eval_result_retrieval.keyword_coverage}% \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d443a95",
   "metadata": {},
   "source": [
    "### RAGAS\n",
    "https://medium.com/data-science/evaluating-rag-applications-with-ragas-81d67b0ee31a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd2f6234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = [test.question for test in tests]\n",
    "ground_truths = [test.ground_truth for test in tests]\n",
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "\"\"\"TODO: reuse the context and generated answer\"\"\"\n",
    "for test in tests:\n",
    "    test_answer, test_context = generate_answer(test.question)\n",
    "    answers.append(test_answer)\n",
    "    # each query have multiple context documents\n",
    "    contexts.append([doc.page_content for doc in test_context])\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"question\": questions,\n",
    "    \"reference\": ground_truths,\n",
    "    \"contexts\": contexts,\n",
    "    \"answer\": answers,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc37a972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q9/87xq5q2d0sv9vrwx1n47nhvc0000gn/T/ipykernel_5705/4164449180.py:2: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import (\n",
      "/var/folders/q9/87xq5q2d0sv9vrwx1n47nhvc0000gn/T/ipykernel_5705/4164449180.py:2: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
      "  from ragas.metrics import (\n",
      "/var/folders/q9/87xq5q2d0sv9vrwx1n47nhvc0000gn/T/ipykernel_5705/4164449180.py:2: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
      "  from ragas.metrics import (\n",
      "/var/folders/q9/87xq5q2d0sv9vrwx1n47nhvc0000gn/T/ipykernel_5705/4164449180.py:2: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import (\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328ea2a76d3d4eddb06719de0ad3397f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "from rag_ingestion import embeddings\n",
    "\n",
    "\"\"\"\n",
    "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
    "is because only have 1 LLM, and RAGAS is expecting 3 by default, but 1 still works \n",
    "\"\"\"\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    "    embeddings=embeddings,\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de3cece8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How many teams benefited from Beiji’s n8n roll...</td>\n",
       "      <td>[Software Engineer Aug 2023 - Present United O...</td>\n",
       "      <td>Beiji's n8n rollout benefited over 10 teams an...</td>\n",
       "      <td>More than 10 teams used the automation workflo...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.760898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which two internal platforms did Beiji build t...</td>\n",
       "      <td>[for 200+ templates. • Built in-house Develope...</td>\n",
       "      <td>Beiji built two internal platforms to improve ...</td>\n",
       "      <td>He built an in-house Developer Portal and work...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.943257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What access control problem did the Prompt Tem...</td>\n",
       "      <td>[for 200+ templates. • Built in-house Develope...</td>\n",
       "      <td>The Prompt Template Hub solved the access cont...</td>\n",
       "      <td>It provided version control and access control...</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.903517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which project of Beiji’s is being scaled by an...</td>\n",
       "      <td>[What truly sets Beiji apart is his innate dri...</td>\n",
       "      <td>The project being scaled by an enterprise GenA...</td>\n",
       "      <td>The LLM-Based Bitbucket Code Reviewer is being...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.717903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What retrieval techniques improved the Best Pr...</td>\n",
       "      <td>[LoRA and AI Agents | Udemy • Fine-tuned Llama...</td>\n",
       "      <td>The retrieval techniques that improved the Bes...</td>\n",
       "      <td>Query rewriting, reranking, and optimized embe...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.992616</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  How many teams benefited from Beiji’s n8n roll...   \n",
       "1  Which two internal platforms did Beiji build t...   \n",
       "2  What access control problem did the Prompt Tem...   \n",
       "3  Which project of Beiji’s is being scaled by an...   \n",
       "4  What retrieval techniques improved the Best Pr...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Software Engineer Aug 2023 - Present United O...   \n",
       "1  [for 200+ templates. • Built in-house Develope...   \n",
       "2  [for 200+ templates. • Built in-house Develope...   \n",
       "3  [What truly sets Beiji apart is his innate dri...   \n",
       "4  [LoRA and AI Agents | Udemy • Fine-tuned Llama...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Beiji's n8n rollout benefited over 10 teams an...   \n",
       "1  Beiji built two internal platforms to improve ...   \n",
       "2  The Prompt Template Hub solved the access cont...   \n",
       "3  The project being scaled by an enterprise GenA...   \n",
       "4  The retrieval techniques that improved the Bes...   \n",
       "\n",
       "                                           reference  context_precision  \\\n",
       "0  More than 10 teams used the automation workflo...           1.000000   \n",
       "1  He built an in-house Developer Portal and work...           1.000000   \n",
       "2  It provided version control and access control...           0.416667   \n",
       "3  The LLM-Based Bitbucket Code Reviewer is being...           0.500000   \n",
       "4  Query rewriting, reranking, and optimized embe...           0.750000   \n",
       "\n",
       "   context_recall  faithfulness  answer_relevancy  \n",
       "0             1.0      1.000000          0.760898  \n",
       "1             1.0      1.000000          0.943257  \n",
       "2             1.0      0.600000          0.903517  \n",
       "3             0.0      0.714286          0.717903  \n",
       "4             1.0      0.714286          0.992616  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a2662606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contect precision:  0.50\n",
      "contect recall:  0.69\n",
      "faithfulness:  0.83\n",
      "answer relevancy:  0.78\n"
     ]
    }
   ],
   "source": [
    "print(\"contect precision: \", format(df[\"context_precision\"].mean(), \".2f\"))\n",
    "print(\"contect recall: \", format(df[\"context_recall\"].mean(), \".2f\"))\n",
    "print(\"faithfulness: \", format(df[\"faithfulness\"].mean(), \".2f\"))\n",
    "print(\"answer relevancy: \", format(df[\"answer_relevancy\"].mean(), \".2f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
