{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21973e47",
   "metadata": {},
   "source": [
    "# RAG evaluator\n",
    "- [x] Metrics -> on Retriever component\n",
    "- [x] Generator component -> with LLM-based judge\n",
    "- [x] evaluation dataset\n",
    "- [x] llm main style evaluation\n",
    "- [x] RAGAS https://medium.com/data-science/evaluating-rag-applications-with-ragas-81d67b0ee31a\n",
    "- [ ] Dashboard\n",
    "- [ ] better source data on detailed project and information\n",
    "- [ ] better golden dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50693432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from base_models import TestQuestion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c66676",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c8f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "TEST_QUESTIONS_FILE = \"../evaluation/eval_data.jsonl\"\n",
    "\n",
    "def load_test_questions() -> list[TestQuestion]:\n",
    "    \"\"\"\n",
    "    Load test questions from a JSONL file\n",
    "    \"\"\"\n",
    "    with open(TEST_QUESTIONS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        tests = []\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip()) \n",
    "            tests.append(TestQuestion(**data))\n",
    "        print(\"Loaded {} test questions\".format(len(tests)))\n",
    "    return tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73109225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20 test questions\n"
     ]
    }
   ],
   "source": [
    "tests = load_test_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b46e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is Beiji's current role and where does he work?\n",
      "Beiji is a Software Engineer at United Overseas Bank (UOB) since Aug 2023.\n",
      "career\n",
      "['Beiji', 'Software Engineer', 'United Overseas Bank', 'UOB', '2023']\n"
     ]
    }
   ],
   "source": [
    "tests[0]\n",
    "print(tests[0].question)\n",
    "print(tests[0].ground_truth)\n",
    "print(tests[0].category)\n",
    "print(tests[0].keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a475d53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'rag_skill': 3,\n",
       "         'skills': 3,\n",
       "         'lifestyle': 3,\n",
       "         'ai_engineering': 2,\n",
       "         'education': 2,\n",
       "         'personality': 2,\n",
       "         'career': 1,\n",
       "         'platform_engineering': 1,\n",
       "         'achievement': 1,\n",
       "         'future': 1,\n",
       "         'engineering': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "count = Counter([t.category for t in tests])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d91f93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval results:\n",
      "[Document(id='77fa1c7c-f3a2-40c7-98d5-510f45f6f5b8', metadata={'doc_type': 'data', 'source': 'hobby.md'}, page_content='Beiji has been playing basketball since middle school, and it remains his favorite sport. During his time at Southeast University, he served as **captain of the Transportation Department basketball team**, leading training sessions, match preparation, and team strategy. Balancing competition with'), Document(id='4a22976a-14a6-4d20-bb9b-6a93ffe4842e', metadata={'source': 'cv.md', 'doc_type': 'data'}, page_content='# LI BEIJI\\n\\nðŸ“ž +65 8432 9134  \\nðŸ”— [LinkedIn]  \\nðŸ“§ libeiji08121999@gmail.com  \\nðŸ’» [GitHub]  \\nðŸŒ [Portfolio]\\n\\n---\\n\\n## SUMMARY'), Document(id='91aed67f-d81f-41e8-8ba8-2f828c7cc8a3', metadata={'doc_type': 'data', 'source': 'hobby.md'}, page_content='Beiji began weightlifting in 2020 and has trained consistently since. By 2024, he achieved personal records of **175kg squat**, **190kg deadlift**, and **120kg bench press**. He appreciates weightlifting for its measurable progress â€” small weekly improvements that add up over time. The discipline'), Document(id='c89c2211-c269-4423-b3ff-75dc46c30026', metadata={'source': 'hobby.md', 'doc_type': 'data'}, page_content='Beiji earned his **Open Water (OW)** and **Advanced Open Water (AOW)** diving licenses in Bali, Indonesia, and later completed the **Rescue Diver Course** in Komodo. For him, diving is an escape from routine and stress â€” a rare environment with no Wi-Fi, no distractions, just calm focus above and'), Document(id='4c573151-aad3-455d-bf06-ef89f1fc90b1', metadata={'source': 'cv.md', 'doc_type': 'data'}, page_content='- Course: Computer Vision and Deep Learning (NUS-Yale)  \\n- Research: Traffic safety monitoring via vehicle location and size tracking from surveillance cameras  \\n\\n---\\n\\n### Southeast University (SEU)  \\n**Bachelor of Engineering**  \\n*Sep 2017 â€“ Jun 2021*'), Document(id='72f904ef-638f-4835-8dfa-ffae33de947c', metadata={'source': 'cv.md', 'doc_type': 'data'}, page_content='---\\n\\n## WORK EXPERIENCE\\n\\n### Software Engineer  \\n**United Overseas Bank (UOB)**  \\n*Aug 2023 â€“ Present*')]\n"
     ]
    }
   ],
   "source": [
    "from rag_retrieval import fetch_context\n",
    "\n",
    "retrieval_results = fetch_context(tests[0].question)\n",
    "\n",
    "print(\"Retrieval results:\")\n",
    "print(retrieval_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e818ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from base_models import RetrievalLLMEval\n",
    "from rag_retrieval import generate_answer\n",
    "\n",
    "LLM_EVAL_PROMPT = \"\"\"\n",
    "You are a helpful assistant that can answer questions about the user's CV and hobbies.\n",
    "You are given a question and a context.\n",
    "You need to evaluate the retrieval results based on the context.\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "Generated answer:\n",
    "{generated_answer}\n",
    "\n",
    "Golden answer:\n",
    "{ground_truth}\n",
    "\n",
    "Evaluation criteria:\n",
    "- Accuracy: How many of the retrieval results are correct?\n",
    "- Relevance: How relevant are the retrieval results to the question?\n",
    "- Completeness: How complete are the retrieval results?\n",
    "- Confidence: How confident are you in the retrieval results?\n",
    "- Score: The average of accuracy, relevance, completeness\n",
    "\n",
    "Return in the following format:\n",
    "{{\n",
    "    \"accuracy\": 3,\n",
    "    \"relevance\": 2,\n",
    "    \"completeness\": 4,\n",
    "    \"confidence\": 0.9,\n",
    "    \"feedback\": \"The retrieval results is not relevant to the question but correct\",\n",
    "    \"score\": 3\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0);\n",
    "\n",
    "def evaluate_response(test_question: TestQuestion) -> RetrievalLLMEval:\n",
    "    \"\"\"\n",
    "    Evaluate the LLM-Response based on the retrieval results, not on the retrieval results based on the question\n",
    "    \"\"\"\n",
    "    # get the context\n",
    "    generated_answer, retrieval_results = generate_answer(test_question.question)\n",
    "\n",
    "    # parse the messages\n",
    "    system_messages = [SystemMessage(\n",
    "        content=(\"You are an expert evaluator assessing the quality of answers. Evaluate the generated answer by comparing it to the reference answer. Only give 5/5 scores for perfect answers.\"\n",
    "                 ))]\n",
    "    user_messages = [HumanMessage(content=LLM_EVAL_PROMPT.format(question=test_question.question, generated_answer=generated_answer, ground_truth=test_question.ground_truth))]\n",
    "    messages = system_messages + user_messages\n",
    "\n",
    "    structured_llm = llm.with_structured_output(RetrievalLLMEval)\n",
    "    response_LLM_eval = structured_llm.invoke(messages)\n",
    "\n",
    "    return response_LLM_eval\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a60d8b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: <class 'base_models.RetrievalLLMEval'>\n",
      "Is RetrievalLLMEval? True\n",
      "\n",
      "Result object:\n",
      "accuracy=4.0 relevance=4.0 completeness=3.0 confidence=0.9 feedback='The retrieval results are mostly correct and relevant, but lack the specific start date mentioned in the golden answer.' score=3.75\n",
      "\n",
      "Access attributes:\n",
      "accuracy: 4.0\n",
      "relevance: 4.0\n",
      "completeness: 3.0\n",
      "score: 3.75\n",
      "confidence: 0.9\n",
      "feedback: The retrieval results are mostly correct and relevant, but lack the specific start date mentioned in the golden answer.\n"
     ]
    }
   ],
   "source": [
    "result = evaluate_response(tests[0])\n",
    "\n",
    "# Check if it's a BaseModel/RetrievalLLMEval instance\n",
    "print(\"Type:\", type(result))\n",
    "print(\"Is RetrievalLLMEval?\", isinstance(result, RetrievalLLMEval))\n",
    "print(\"\\nResult object:\")\n",
    "print(result)\n",
    "print(\"\\nAccess attributes:\")\n",
    "print(f\"accuracy: {result.accuracy}\")\n",
    "print(f\"relevance: {result.relevance}\")\n",
    "print(f\"completeness: {result.completeness}\")\n",
    "print(f\"score: {result.score}\")\n",
    "print(f\"confidence: {result.confidence}\")\n",
    "print(f\"feedback: {result.feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f41356b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_LLM(tests: list[TestQuestion]) -> RetrievalLLMEval:\n",
    "    \"\"\"\n",
    "    Evaluate all the tests\n",
    "    \"\"\"\n",
    "    results = []  \n",
    "    for test in tests:\n",
    "        results.append(evaluate_response(test))\n",
    "    evaluation_result = RetrievalLLMEval(\n",
    "        accuracy=sum([result.accuracy for result in results]) / len(results),\n",
    "        relevance=sum([result.relevance for result in results]) / len(results),\n",
    "        completeness=sum([result.completeness for result in results]) / len(results),\n",
    "        score=sum([result.score for result in results]) / len(results),\n",
    "        confidence=sum([result.confidence for result in results]) / len(results),\n",
    "        feedback=\"This is the average of all the tests\",\n",
    "    )\n",
    "    return evaluation_result\n",
    "\n",
    "eval_result_LLM = evaluate_LLM(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average of all the tests from LLM evaluation on LLM answer:\n",
      "accuracy: 3.9\n",
      "relevance: 3.35\n",
      "completeness: 4.15\n",
      "score: 3.8625\n",
      "confidence: 0.89\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAverage of all the tests from LLM evaluation on LLM answer:\")\n",
    "print(f\"accuracy: {eval_result_LLM.accuracy}\")\n",
    "print(f\"relevance: {eval_result_LLM.relevance}\")\n",
    "print(f\"completeness: {eval_result_LLM.completeness}\")\n",
    "print(f\"score: {eval_result_LLM.score}\")\n",
    "print(f\"confidence: {eval_result_LLM.confidence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a0101",
   "metadata": {},
   "source": [
    "### The metric based evals on RAG retrieval result\n",
    "- MRR\n",
    "- Keyword Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ded30ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mrr(keyword:str, retrieval_results:list) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the MRR of the retrieval results,\n",
    "    mrr = 1 -> first result contains the keyword\n",
    "    mrr = 0.5 -> second result contains the keyword\n",
    "    mrr = 0 -> no result contains the keyword\n",
    "    \"\"\"\n",
    "    keyword = keyword.lower();\n",
    "    for rank, result in enumerate(retrieval_results, start=1):\n",
    "        if keyword in result.page_content.lower():\n",
    "            return 1/rank\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd608c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_models import RetrievalEval\n",
    "\n",
    "\n",
    "def evaluate_retrieval(test: TestQuestion) -> RetrievalEval:\n",
    "    \"\"\"\n",
    "    Evaluate the retrieval results\n",
    "    \"\"\"\n",
    "\n",
    "    retrieved_docs = fetch_context(test.question)\n",
    "    mrr_scores = [evaluate_mrr(keyword, retrieved_docs) for keyword in test.keywords]# each keyword need to be calculated separately, so a list of scores\n",
    "    avg_mrr = sum(mrr_scores) / len(mrr_scores) if mrr_scores else 0.0\n",
    "\n",
    "    # Calculate keyword coverage\n",
    "    keywords_found = sum(1 for score in mrr_scores if score > 0)\n",
    "    total_keywords = len(test.keywords)\n",
    "    keyword_coverage = (keywords_found / total_keywords * 100) if total_keywords > 0 else 0.0\n",
    "\n",
    "    return RetrievalEval(\n",
    "        MRR=avg_mrr,\n",
    "        keyword_coverage=keyword_coverage,\n",
    "    )\n",
    "\n",
    "def evaluate_all(tests: list[TestQuestion]) -> RetrievalEval:\n",
    "    \"\"\"\n",
    "    Evaluate all the tests\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for test in tests:\n",
    "        results.append(evaluate_retrieval(test))\n",
    "    \n",
    "    mrr_final = sum(result.MRR for result in results) / len(results)\n",
    "    keyword_coverage_final = sum(result.keyword_coverage for result in results) / len(results)\n",
    "\n",
    "    return RetrievalEval(\n",
    "        MRR=format(mrr_final, \".2f\"),\n",
    "        keyword_coverage=format(keyword_coverage_final, \".2f\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d69b6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average of all the tests from retrieval evaluation:\n",
      "MRR: 0.54\n",
      "Keyword Coverage: 79.67% \n"
     ]
    }
   ],
   "source": [
    "eval_result_retrieval = evaluate_all(tests)\n",
    "print(\"\\nAverage of all the tests from retrieval evaluation:\")\n",
    "print(f\"MRR: {eval_result_retrieval.MRR}\")\n",
    "print(f\"Keyword Coverage: {eval_result_retrieval.keyword_coverage}% \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d443a95",
   "metadata": {},
   "source": [
    "### RAGAS\n",
    "https://medium.com/data-science/evaluating-rag-applications-with-ragas-81d67b0ee31a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd2f6234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "questions = [test.question for test in tests]\n",
    "ground_truths = [test.ground_truth for test in tests]\n",
    "contexts = []\n",
    "answers = []\n",
    "\n",
    "for test in tests:\n",
    "    test_answer, test_context = generate_answer(test.question)\n",
    "    answers.append(test_answer)\n",
    "    # each query have multiple context documents\n",
    "    contexts.append([doc.page_content for doc in test_context])\n",
    "\n",
    "dataset = Dataset.from_dict({\n",
    "    \"question\": questions,\n",
    "    \"reference\": ground_truths,\n",
    "    \"contexts\": contexts,\n",
    "    \"answer\": answers,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc37a972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q9/87xq5q2d0sv9vrwx1n47nhvc0000gn/T/ipykernel_9246/2051731147.py:2: DeprecationWarning: Importing faithfulness from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import faithfulness\n",
      "  from ragas.metrics import (\n",
      "/var/folders/q9/87xq5q2d0sv9vrwx1n47nhvc0000gn/T/ipykernel_9246/2051731147.py:2: DeprecationWarning: Importing answer_relevancy from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import answer_relevancy\n",
      "  from ragas.metrics import (\n",
      "/var/folders/q9/87xq5q2d0sv9vrwx1n47nhvc0000gn/T/ipykernel_9246/2051731147.py:2: DeprecationWarning: Importing context_recall from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_recall\n",
      "  from ragas.metrics import (\n",
      "/var/folders/q9/87xq5q2d0sv9vrwx1n47nhvc0000gn/T/ipykernel_9246/2051731147.py:2: DeprecationWarning: Importing context_precision from 'ragas.metrics' is deprecated and will be removed in v1.0. Please use 'ragas.metrics.collections' instead. Example: from ragas.metrics.collections import context_precision\n",
      "  from ragas.metrics import (\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80cfb4dc2cbe4ce5888335bf683aac83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
      "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n"
     ]
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "from rag_ingestion import embeddings\n",
    "\n",
    "result = evaluate(\n",
    "    dataset = dataset, \n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        context_recall,\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "    ],\n",
    "    embeddings=embeddings,\n",
    ")\n",
    "\n",
    "df = result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de3cece8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is Beiji's current role and where does he...</td>\n",
       "      <td>[Beiji has been playing basketball since middl...</td>\n",
       "      <td>Beiji's current role is a Software Engineer at...</td>\n",
       "      <td>Beiji is a Software Engineer at United Oversea...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What impact did Beiji make when rolling out n8n?</td>\n",
       "      <td>[- Led adoption and production rollout of **n8...</td>\n",
       "      <td>Beiji led the adoption and production rollout ...</td>\n",
       "      <td>He led the production rollout of n8n enabling ...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.890352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the Prompt Template Hub that Beiji built?</td>\n",
       "      <td>[- Designed and implemented **Prompt Template ...</td>\n",
       "      <td>The Prompt Template Hub that Beiji built is a ...</td>\n",
       "      <td>It is a React and Spring Boot platform integra...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Describe Beiji's experience with developer pla...</td>\n",
       "      <td>[- Built an in-house **Developer Portal** (Ang...</td>\n",
       "      <td>Beiji has significant experience with develope...</td>\n",
       "      <td>He built an in-house Developer Portal using An...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is Beijiâ€™s strongest RAG-related project?</td>\n",
       "      <td>[Beiji began weightlifting in 2020 and has tra...</td>\n",
       "      <td>Beiji's strongest RAG-related project is the \"...</td>\n",
       "      <td>The Best Price Notification Agent project wher...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.988692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What is Beiji's current role and where does he...   \n",
       "1   What impact did Beiji make when rolling out n8n?   \n",
       "2  What is the Prompt Template Hub that Beiji built?   \n",
       "3  Describe Beiji's experience with developer pla...   \n",
       "4     What is Beijiâ€™s strongest RAG-related project?   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [Beiji has been playing basketball since middl...   \n",
       "1  [- Led adoption and production rollout of **n8...   \n",
       "2  [- Designed and implemented **Prompt Template ...   \n",
       "3  [- Built an in-house **Developer Portal** (Ang...   \n",
       "4  [Beiji began weightlifting in 2020 and has tra...   \n",
       "\n",
       "                                            response  \\\n",
       "0  Beiji's current role is a Software Engineer at...   \n",
       "1  Beiji led the adoption and production rollout ...   \n",
       "2  The Prompt Template Hub that Beiji built is a ...   \n",
       "3  Beiji has significant experience with develope...   \n",
       "4  Beiji's strongest RAG-related project is the \"...   \n",
       "\n",
       "                                           reference  context_precision  \\\n",
       "0  Beiji is a Software Engineer at United Oversea...           0.166667   \n",
       "1  He led the production rollout of n8n enabling ...           1.000000   \n",
       "2  It is a React and Spring Boot platform integra...           1.000000   \n",
       "3  He built an in-house Developer Portal using An...           1.000000   \n",
       "4  The Best Price Notification Agent project wher...           0.000000   \n",
       "\n",
       "   context_recall  faithfulness  answer_relevancy  \n",
       "0             1.0      1.000000          0.914340  \n",
       "1             1.0      1.000000          0.890352  \n",
       "2             1.0      0.666667          1.000000  \n",
       "3             1.0      1.000000          0.871991  \n",
       "4             0.0      0.888889          0.988692  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
